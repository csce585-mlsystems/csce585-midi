# csce585-midi

## Group Info  
- Student_1 Thomas Kareka  
	- Email: tkareka@email.sc.edu 
- Student_2 Cade Stocker 
	- Email: cstocker@email.sc.edu  

## Project Summary/Abstract  

## Problem Description  
Our group is interested in AI music generation via MIDI files. MIDI files contain instructions, rather than audio data, which tell software how to play a song. These instructions are contained in chunks, containing event data such as notes and control changes. Despite not being human readable, MIDI data is easily translatable into a variety of formats, and is used as the core for Digital Audio Workstation editors. Although AI models such as MusicLM exist to generate music, these create raw audio in the form of waveforms. As such, it is very hard for a user to iterate upon its creations, as changes would require the entire waveform to be regenerated. The use of MIDI allows for small, incremental tweaks, while still keeping the end user as part of the process through their DAW.
- Motivation  
	- Interest in both music and machine learning  
	- MIDI allows for incremental, user-driven editing  
- Challenges  
	- Limited genre/emotion labels in XMIDI dataset  
	- Need to supplement data with less-labeled datasets  
	- Translating text descriptions to structured MIDI data  

## Contribution  

## References   

Zhu, Y., Baca, J., Rekabdar, B., Rawassizadeh, R. (2023). A Survey of AI Music Generation Tools and Models. [arXiv:2308.12982](https://arxiv.org/abs/2308.12982)

Briot, J., Hadjeres, G., Pachet, F. (2017). Deep Learning Techniques for Music Generation -- A Survey.[arXiv:1709.01620](https://arxiv.org/abs/1709.01620)

Bhandari, K., Roy, A., Wang, K., Puri, G., Colton, S., Herremans, D. (2024). Text2midi: Generating Symbolic Music from Captions. [arXiv:2412.16526](https://arxiv.org/abs/2412.16526)

Yang, L., Chou, S., Yang, Y. (2017). MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation. [arXiv:1703.10847](https://arxiv.org/abs/1703.10847)

Tian, S., Zhang, C., Yuan, W., Tan, W., Zhu, W. (2025). XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework. [arXiv:2501.08809](https://arxiv.org/abs/2501.08809)

Colin Raffel. **"Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching"**. _PhD Thesis_, 2016. https://colinraffel.com/publications/thesis.pdf

---

## Dependencies  

## Directory Structure  

## How to Run  

## Demo  
---
